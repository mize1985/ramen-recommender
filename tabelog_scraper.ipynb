{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes review comments from Tabelog. It finds the top available shops for a certain category in a certain region with a certain search term(which doesn't seem to actually do anything. 風 is used as a placeholder for the search term. Other types of regions and types of restaurant work in the scraper class, but we're only using prefectures and ramen shops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scraper():\n",
    "    def __init__(self,region,genre,term):\n",
    "        self.region=region\n",
    "        self.term=term\n",
    "        self.genre=genre\n",
    "        url=(f'https://tabelog.com/{self.region}/rstLst/{self.genre}/1/?SrtT=rt&sk={self.term}&Srt=D&sort_mode=1')\n",
    "        response = requests.get(url)\n",
    "        soup = bs(response.text,'lxml')\n",
    "        self.shop_count = int(soup.find_all('span',class_='c-page-count__num')[2].find('strong').contents[0])\n",
    "        self.page_limit = self.shop_count//20\n",
    "        if self.shop_count%20 != 0: self.page_limit +=1\n",
    "        self.page_limit = min(60,self.page_limit)\n",
    "        self.urls=[]\n",
    "        self.reviews={}\n",
    "        self.review_file = f'{self.region}_{self.genre}_{self.term}.pickle'\n",
    "    \n",
    "    def search(self,num_pages):\n",
    "        if num_pages > self.page_limit:\n",
    "            num_pages = self.page_limit\n",
    "            print(f'Number of pages requested is beyond the number available for {self.term} in {self.region}.')\n",
    "            print(f'{self.page_limit} pages will be returned, totalling {min(self.shop_count,1200)} shops.')\n",
    "        for page_number in range(num_pages):\n",
    "            url=(f'https://tabelog.com/{self.region}/rstLst/{self.genre}/{page_number}/?SrtT=rt&sk={self.term}&Srt=D&sort_mode=1')\n",
    "            response = requests.get(url)\n",
    "            soup = bs(response.text,'lxml')\n",
    "            for h4 in soup.find_all('h4',class_='list-rst__rst-name'):\n",
    "                self.urls.append(h4.find('a',\n",
    "                    class_='list-rst__rst-name-target cpy-rst-name js-ranking-num').get('href'))\n",
    "\n",
    "    def get_reviews(self):\n",
    "        start_time = time.time()\n",
    "        for num,url in enumerate(self.urls):\n",
    "            elapsed = time.time()-start_time\n",
    "            print(f'After {elapsed}, {num} shops are done.')\n",
    "            if num != 0: print(f'{elapsed*(len(self.urls)-num)/num} is estimated to remain.')\n",
    "            self.reviews[num] = scrape_reviews(url)\n",
    "        with open(self.review_file, 'wb') as to_write:\n",
    "            pickle.dump((self.reviews,self.urls), to_write)\n",
    "        to_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment prefectures as needed/wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefectures = ['tokyo',\n",
    "    #'kanagawa','osaka','saitama','chiba','hyogo','fukuoka','shizuoka',\n",
    "    #'ibaraki','hiroshima','kyoto','niigata','miyagi','tochigi','fukushima',\n",
    "    #'okayama','kumamoto','kagoshima','yamaguchi','ehime','nagasaki','shiga','nara','aomori',\n",
    "    #'iwate','oita','ishikawa','yamagata','miyazaki','toyama','akita','wakayama','kagawa','saga',\n",
    "    'aichi','fukui','yamanashi','hokkaido','nagano','gifu','gunma','mie','okinawa']\n",
    "    'shizuoka','niigata','toyama','ishikawa',]\n",
    "    #'tokushima','kochi','shimane','tottori']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefecture in prefectures:\n",
    "    now = scraper(prefecture,'ramen','風')\n",
    "    now.search(200)\n",
    "    now.get_reviews()\n",
    "    del now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_local = threading.local()\n",
    "\n",
    "\n",
    "def get_session():\n",
    "    if not hasattr(thread_local, \"session\"):\n",
    "        thread_local.session = requests.Session()\n",
    "    return thread_local.session\n",
    "\n",
    "def scrape_reviews(url):\n",
    "    global reviews\n",
    "    reviews = []\n",
    "    #f = open(\"/media/sam/a97ba62c-d723-4b41-8b12-883a65db419b/tmp.txt\", \"w\")\n",
    "    #f.write(\"\")\n",
    "    #f.close()\n",
    "    reviews_page = url+'/dtlrvwlst/?use_type=0&rvw_part=all&lc=1'\n",
    "    response = requests.get(reviews_page)\n",
    "    soup = bs(response.text,'lxml')\n",
    "    review_links = soup.find_all('a',class_='rvw-item__title-target')\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        executor.map(scrape_page,review_links)\n",
    "    return reviews.copy()\n",
    "    #f = open(\"/media/sam/a97ba62c-d723-4b41-8b12-883a65db419b/tmp.txt\", \"r\")\n",
    "    #return(f.read()) \n",
    "        \n",
    "def scrape_page(review_link):\n",
    "    review_response = requests.get('https://www.tabelog.com/'+review_link.get('href'))\n",
    "    review_soup = bs(review_response.text, 'lxml')\n",
    "    text_lines = review_soup.find('div',class_='rvw-item__rvw-comment').find('p').contents\n",
    "    review=''\n",
    "    for line in text_lines:\n",
    "        review+=re.sub(r'(<[^>]*>|[[:space:]]|\\\\n|\\.|\\/)','',str(line))\n",
    "    reviews.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part was added after the rest and run separately to get more data. In the future, it could probably be part of the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_local = threading.local()\n",
    "\n",
    "identity={}\n",
    "\n",
    "def get_session():\n",
    "    if not hasattr(thread_local, \"session\"):\n",
    "        thread_local.session = requests.Session()\n",
    "    return thread_local.session\n",
    "\n",
    "def identity_scraper(url):\n",
    "        \n",
    "    response=requests.get(url)\n",
    "    soup = bs(response.text,'lxml')\n",
    "    name = soup.find('h2',class_='display-name').find('span').contents[0].replace('\\n',''\n",
    "        ).replace(' ','')\n",
    "    rating = float(soup.find('span',class_='rdheader-rating__score-val-dtl'\n",
    "                    ).contents[0])\n",
    "    (lat,long)=soup.find_all('img',class_='js-map-lazyload rstinfo-table__map-image'\n",
    "         )[0].get('data-original').split('&')[4].split('=')[1].split(',')\n",
    "    identity[url]=[name,rating,lat,long]\n",
    "prefecture_list = ['aichi','fukui','yamanashi','nagano','gifu',\n",
    "                   'shizuoka','niigata','toyama','ishikawa']\n",
    "for prefecture in prefecture_list:\n",
    "    print(prefecture)\n",
    "    try:\n",
    "        with open(f'{prefecture}_ramen_風.pickle', 'rb') as to_read:\n",
    "            reviews_dict,urls = pickle.load(to_read)\n",
    "        to_read.close()\n",
    "    except:\n",
    "        with open(f'{prefecture}_ramen_風.pickle', 'rb') as to_read:\n",
    "            urls = pickle.load(to_read)\n",
    "        to_read.close()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        executor.map(identity_scraper,urls)\n",
    "    with open('identity.pickle', 'wb') as to_write:\n",
    "        pickle.dump(identity, to_write)\n",
    "    to_write.close()\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
